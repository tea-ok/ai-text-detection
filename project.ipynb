{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tea-ok/miniconda3/envs/classification-project/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import random_split\n",
    "from transformers import BertTokenizer\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  generated\n",
       "0  Cars. Cars have been around since they became ...        0.0\n",
       "1  Transportation is a large necessity in most co...        0.0\n",
       "2  \"America's love affair with it's vehicles seem...        0.0\n",
       "3  How often do you ride in a car? Do you drive a...        0.0\n",
       "4  Cars are a wonderful thing. They are perhaps o...        0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/AI_Human.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your original dataframe\n",
    "df_zero = df[df['generated'] == 0]\n",
    "df_one = df[df['generated'] == 1]\n",
    "\n",
    "# Sample 5000 rows from each dataframe\n",
    "df_zero_sampled = df_zero.sample(5000, random_state=1)\n",
    "df_one_sampled = df_one.sample(5000, random_state=1)\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df = pd.concat([df_zero_sampled, df_one_sampled])\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length: 1642\n"
     ]
    }
   ],
   "source": [
    "# Determining the max length (in words) of rows of the data\n",
    "maxlen = df['text'].apply(lambda x: len(x.split())).max()\n",
    "print(f'Max length: {maxlen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = dataframe.generated\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        # Calculate the length of the sequence before padding\n",
    "        text_length = len([token for token in ids if token != self.tokenizer.pad_token_id])\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n",
    "            'text_lengths': torch.tensor(text_length, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create the Dataset\n",
    "dataset = TextDataset(df, tokenizer, max_len=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(dataset, train_prop=0.8, val_prop=0.1, test_prop=0.1, batch_size=128):\n",
    "    train_len = int(train_prop * len(dataset))\n",
    "    val_len = int(val_prop * len(dataset))\n",
    "    test_len = int(test_prop * len(dataset))\n",
    "\n",
    "    train_set, val_set, test_set = random_split(dataset, [train_len, val_len, test_len])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_data_loaders(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, dimension=32):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dimension = dimension\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim,\n",
    "                            hidden_size=dimension,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=False)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        # self.fc = nn.Linear(2*dimension, 1)\n",
    "        self.fc = nn.Linear(dimension, 1)\n",
    "\n",
    "    def forward(self, text, text_len):\n",
    "\n",
    "        text_emb = self.embedding(text)\n",
    "\n",
    "        packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        # out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "        # out_reverse = output[:, 0, self.dimension:]\n",
    "        # out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
    "        # text_fea = self.drop(out_reduced)\n",
    "\n",
    "        out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
    "        text_fea = self.drop(out_forward)\n",
    "\n",
    "        text_fea = self.fc(text_fea)\n",
    "        text_fea = torch.squeeze(text_fea, 1)\n",
    "        text_out = torch.sigmoid(text_fea)\n",
    "\n",
    "        return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "embedding_dim = 100\n",
    "\n",
    "print(f'Vocab size: {vocab_size}')\n",
    "\n",
    "# Create the model\n",
    "model = LSTMClassifier(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set device to GPU if it is, else CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Defining the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Moving the model and loss function to same device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch')\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Sort the sequences by length in descending order\n",
    "        text_lengths, sorted_idx = batch['text_lengths'].sort(descending=True)\n",
    "        ids = batch['ids'][sorted_idx]\n",
    "        targets = batch['targets'][sorted_idx]\n",
    "\n",
    "        ids = ids.to(device)\n",
    "        text_lengths = text_lengths.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(ids, text_lengths)\n",
    "        loss = criterion(predictions, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item())})\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, total_correct, total_labels = 0, 0, 0\n",
    "        for batch in val_loader:\n",
    "            text_lengths, sorted_idx = batch['text_lengths'].sort(descending=True)\n",
    "            ids = batch['ids'][sorted_idx]\n",
    "            targets = batch['targets'][sorted_idx]\n",
    "\n",
    "            ids = ids.to(device)\n",
    "            text_lengths = text_lengths.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            predictions = model(ids, text_lengths)\n",
    "            loss = criterion(predictions, targets)\n",
    "\n",
    "            total_loss += loss.item() * ids.size(0)\n",
    "            total_correct += (predictions.round() == targets).sum().item()\n",
    "            total_labels += ids.size(0)\n",
    "\n",
    "        avg_loss = total_loss / total_labels\n",
    "        avg_acc = total_correct / total_labels\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()), 'validation_loss': '{:.3f}'.format(avg_loss), \n",
    "                                  'validation_accuracy': '{:.3f}'.format(avg_acc)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving in Colab\n",
    "# path = \"/content/drive/My Drive/AI classification/model.pth\"\n",
    "\n",
    "# # Save the model\n",
    "# torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding): Embedding(30522, 100)\n",
       "  (lstm): LSTM(100, 32, batch_first=True)\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading saved model\n",
    "path = 'model.pth'\n",
    "model = LSTMClassifier(len(tokenizer.vocab), 100)\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/tea-ok/miniconda3/envs/classification-project/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the list to store the targets and predictions\n",
    "all_targets = []\n",
    "all_predictions = []\n",
    "\n",
    "# Start the evaluation loop\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Sort the sequences by length in descending order\n",
    "        text_lengths, sorted_idx = batch['text_lengths'].sort(descending=True)\n",
    "        ids = batch['ids'][sorted_idx]\n",
    "        targets = batch['targets'][sorted_idx]\n",
    "\n",
    "        ids = ids.to(device)\n",
    "        text_lengths = text_lengths.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(ids, text_lengths).squeeze()\n",
    "\n",
    "        # Convert the predictions and targets to the same data type and device\n",
    "        predictions = torch.round(torch.sigmoid(predictions)).cpu().numpy()\n",
    "        targets = targets.cpu().numpy()\n",
    "\n",
    "        # Store the targets and predictions\n",
    "        all_targets.extend(targets)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(all_targets, all_predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classification-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
